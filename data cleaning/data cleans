from os import read
import re 
import fileinput
from typing import Iterator
import os  



def remove_variables(tweet: str) -> str:
    '''
    Takes a tweet as a string and removes unnecessary variables
    :param tweet: string representation of tweet
    :returns: a string containing the cleaned version of the tweet
    '''

    variables_to_remove: list[str] = ["source", "location", "url", "protected", "utc_offset", "time_zone", "geo_enabled", "contributors_enabled", "is_translator", "profile_background_color", "profile_background_image_url", "profile_background_image_url_https", "profile_background_tile", "profile_link_color", "profile_sidebar_border_color", "profile_sidebar_fill_color", "profile_text_color", "profile_use_background_image", "profile_image_url", "profile_image_url_https", "profile_banner_url", "default_profile", "default_profile_image", "following", "follow_request_sent", "notifications"]

    # Removes the following variables from tweet string:
    for variable in variables_to_remove:
        regex = r'\"' + variable + r'\":[^,]+,'
        tweet = re.sub(regex, "", tweet)

    return tweet



def make_tweet_list(path: str)-> list[str]: 
    """
    Reads the data from the file located at path
    :param path: path to the data to be loaded
    :returns: list with a tweets
    """
    with open(path, 'r') as file:
       lines = file.readlines()
    return lines



def check_tweet(tweet: str) -> bool:
    """
    Checks whether the tweet should be kept by looking at media and language
    :param tweet: string representation of tweet
    :returns: A boolean value representing wether we want to keep the tweet in the data
    """
    media_regex = '\"media\":'
    en_lang_regex = '\"lang\":\"en\",'

    # Check whether the tweet has media
    if len(re.findall(media_regex, tweet)) > 0:
        return False
    # Check whether the tweet is in english
    elif len(re.findall(en_lang_regex, tweet)) > 0:
        return True
    else:
        return False
    



def clean_file(old_path: str, new_path:str) -> None:
    """
    Replaces the json file from old_path with a cleaned version of the file at new_path
    :param old_path: the path to the file that is to be cleaned
    :param new_path: the path where the new cleaned file will be generated
    """
    # Get each individual line from the file
    with open(old_path, 'r') as file:
       lines = file.readlines()

    # Write the cleaned version of the lines into the new file
    with open(new_path, 'a') as new_file:
        for number, line in enumerate(lines):

            # Checks if the tweet should be kept and if so adds it to the new file
            if check_tweet(line) == True:
                new_file.write(remove_variables(line))
            else:
                new_file.write('')

#clean_file('data cleaning/../data/airlines-1558527599826.json', 'data cleaning/new file.json')


def find_duplicate(tweet: str, path: str) -> list:
    '''
    Finds duplicate tweets in a list of tweets as strings and returns there index on a list
    :param tweet: the tweet you want to find duplicates of
    :param lines: list with all tweets
    '''
    text_regex = '\"text\":\"[^\"]+\",'
    user_name_regex = '\"name\":[^,]+,'
    tweet_id_regex = '\"id\":.+,'
    timestamp_ms_regex = '\"timestamp_ms\":.+'
    
    # Initialize a list for keeping track of duplicates  
    id_duplicates = list()
    
    lines = make_file_iterator(path)
    # Iterate through all lines
    for line in lines:        
        # Checks if it is a different tweet in the json file
        if str(re.search(tweet_id_regex, tweet)) != str(re.search(tweet_id_regex, line)):
            
            # Checks whether the other tweet is a duplicate, if so add it to the duplicates list
            if str(re.search(text_regex, tweet)) == str(re.search(text_regex, line)) and str(re.search(user_name_regex, tweet)) == str(re.search(user_name_regex, line)) and re.findall(timestamp_ms_regex, tweet)[-1] == re.findall(timestamp_ms_regex, line)[-1]:
                id_duplicates.append(str(re.search(tweet_id_regex, line)))
        
    return id_duplicates
    

def is_retweet(line: str) -> bool:
    """
    Tests if tweet is a retweet
    :param line: tweet in string representation
    """
    rt_regex = '\"text\":\"RT '
    #checks if text starts with RT to signal that it is a retweet
    if len(re.findall(rt_regex, line)) > 0:
        return True
    return False

def make_file_iterator(path: str) -> Iterator:
    """
    Replaces the json file from old_path with a cleaned version of the file at new_path
    :param old_path: the path to the file that is to be cleaned
    :param new_path: the path where the new cleaned file will be generated
    """
    # Get each individual line from the file

    return fileinput.input(path)
    
#print(find_duplicate_ids(TEST_clean_file('data cleaning/test tweet 3')))
#print(find_duplicate(TEST_clean_file('data cleaning/test tweet 3')[1], TEST_clean_file('data cleaning/test tweet 3')))
#print(find_duplicate(TEST_clean_file('data cleaning/test tweet 3')[0], TEST_clean_file('data cleaning/test tweet 3')))

def clean_all_files(path: str) -> None:
    '''
    Cleans all data and adds it to a big file
    :param path: path to the data folder
    '''
    tweet_id_regex = '\"id\":[0-9]+,'
    file_path_list = file_paths_list(path)
    bad_tweets: list = ids_to_remove(path)
    for file_path in file_path_list:
        lines = make_tweet_list(file_path)
        with open(f'{path}/airline_data.json', 'a') as new_file:
            for tweet in lines:

                # Checks if the tweet should be kept and if so adds it to the new file
                if str(re.search(tweet_id_regex, tweet)) in bad_tweets:
                    new_file.write('')
                else:
                    new_file.write(remove_variables(tweet))


def file_paths_list(path_to_data_folder: str) -> list:
    file_path_list: list = list()
    file_names_list: list = os.listdir(path_to_data_folder)
    for file_name in file_names_list:
        file_path_list.append(f'{path_to_data_folder}/{file_name}')  
    return file_path_list





def ids_to_remove(file_path_list: list[str]) -> list[str]:
    '''
    Returns a list with all ids of all tweets to remove
    :param path_to_data_folder: path to you personal data folder
    '''


    ids_to_remove: list = list()
    text_regex = '\"text\":\"[^\"]+\",'
    user_name_regex = '\"name\":[^,]+,'
    tweet_id_regex = '\"id\":[0-9]+,'
    timestamp_ms_regex = '\"timestamp_ms\":.+'
    #go over every file
    for file_path in file_path_list:
        #go over every tweet in that file
        for tweet in make_tweet_list(file_path):
            #checks if tweet needs to be compared
            if check_tweet(tweet):
                #go over every file to compare them
                for file_path_2 in file_path_list:
                    #check every line
                    for line in make_tweet_list(file_path_2):

                    # Checks if it is a different tweet
                        if str(re.search(tweet_id_regex, tweet)) != str(re.search(tweet_id_regex, line)) and not str(re.search(tweet_id_regex, tweet)) in ids_to_remove:

                            # Checks whether the other tweet is a duplicate, if so add it to the duplicates list
                            if str(re.search(text_regex, tweet)) == str(re.search(text_regex, line)) and str(re.search(user_name_regex, tweet)) == str(re.search(user_name_regex, line)) and re.findall(timestamp_ms_regex, tweet)[-1] == re.findall(timestamp_ms_regex, line)[-1]:
                                ids_to_remove.append(str(re.search(tweet_id_regex, line)))
            else:
                ids_to_remove.append(str(re.search(tweet_id_regex, tweet)))

    return ids_to_remove

clean_all_files('data cleaning/../Test data')