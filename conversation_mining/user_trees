import pymongo
from pymongo import MongoClient, ASCENDING
from tqdm import tqdm
import threading
from queue import Queue
from treelib import Tree

# Connect to MongoDB
client = MongoClient("mongodb://localhost:27017/")
db = client.AirplaneMode
user_convo_starters = db.user_convo_starters
replies = db.replies
user_trees = db.user_trees

# Create indexes to speed up the search
user_convo_starters.create_index([("id_str", ASCENDING)])
replies.create_index([("in_reply_to_status_id_str", ASCENDING)])

# Function to build tree using treelib
def build_tree(tweet):
    tree = Tree()
    tree.create_node(tweet['id_str'], tweet['id_str'], data=tweet)
    
    def add_children(parent_id):
        children = replies.find({"in_reply_to_status_id_str": parent_id})
        for child in children:
            child_id = child['id_str']
            tree.create_node(child_id, child_id, parent=parent_id, data=child)
            add_children(child_id)
    
    add_children(tweet['id_str'])
    
    # Return the tree
    return tree

# Function to process a batch of tweets
def process_batch(batch):
    error_count = 0
    for tweet in batch:
        try:
            tree = build_tree(tweet)
            if len(tree.nodes) > 1:
                tree_dict = tree.to_dict(with_data=True)
                user_trees.insert_one({"tree_id": tweet['id_str'], "tree_data": tree_dict})
        except pymongo.errors.OperationFailure as e:
            error_count += 1
            print(f"Error storing tree for tweet {tweet['id_str']}: {e}")
    
    return error_count

# Thread worker function
def worker():
    while True:
        batch = queue.get()
        if batch is None:
            break
        error_count = process_batch(batch)
        error_queue.put(error_count)
        queue.task_done()

# Initialize queue and threading
queue = Queue()
error_queue = Queue()
threads = []
num_threads = 4

for i in range(num_threads):
    thread = threading.Thread(target=worker)
    thread.start()
    threads.append(thread)

# Main loop to process tweets in batches
batch_size = 10000
total_tweets = user_convo_starters.count_documents({})
batches = []

with tqdm(total=total_tweets, desc="Processing tweets") as pbar:
    for i in range(0, total_tweets, batch_size):
        batch = list(user_convo_starters.find().skip(i).limit(batch_size))
        queue.put(batch)
        pbar.update(len(batch))
    
    queue.join()

# Stop workers
for i in range(num_threads):
    queue.put(None)
for thread in threads:
    thread.join()

# Calculate total error count
total_errors = sum([error_queue.get() for _ in range(num_threads)])

print(f"Processing complete with {total_errors} errors.")
